{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SZAftabi/UseRQE/blob/main/Step1_TagClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfhcCzczV-Gk"
      },
      "source": [
        "<center> <font size='6'> ðŸ’Ÿ <b> UseRQE </b> ðŸ’Ÿ </font> </center>\n",
        "<br> <center>Recognizing Question Entailment with User Background-knowledge Modeling\n",
        "<br> <font color='red' size='4'> <b> Step (1) </b> Hierarchical tag clustering </font> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEdGg56FWsWV"
      },
      "source": [
        "# ðŸŒž **mount the drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KphPULSmdv_F"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiXiD59XAIjV"
      },
      "outputs": [],
      "source": [
        "Drive_path = \"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp9kopa_Wv5C"
      },
      "source": [
        "# ðŸŒž **1. requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edGkeM95KrsR"
      },
      "outputs": [],
      "source": [
        "!git clone -q https://github.com/rapidsai/rapidsai-csp-utils.git                   # Fast t-SNE\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n",
        "!pip install -q gensim\n",
        "!pip install -q node2vec\n",
        "!pip install -q networkx\n",
        "!pip install -q scikit-learn-extra\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e18XsaaD8EqX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import cudf\n",
        "import cuml\n",
        "import copy\n",
        "import random\n",
        "import pickle\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import plotly.io as pio\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim.downloader as api\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "cuml.__version__\n",
        "from textblob import Word\n",
        "from node2vec import Node2Vec\n",
        "from cuml.manifold import TSNE\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import repeat\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "from scipy.cluster.hierarchy import (\n",
        "    dendrogram,\n",
        "    linkage,\n",
        "    fcluster\n",
        "    )\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    davies_bouldin_score,\n",
        "    calinski_harabasz_score\n",
        "    )\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    precision_recall_fscore_support\n",
        "    )\n",
        "from transformers import (\n",
        "    BertModel,\n",
        "    AutoModel,\n",
        "    BertTokenizer,\n",
        "    AutoTokenizer\n",
        "    )\n",
        "\n",
        "import torch\n",
        "torch.cuda.is_available()\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SMfdI5YAMB2"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/VenkateshwaranB/stellargraph.git\n",
        "import stellargraph as sg\n",
        "from stellargraph.data import EdgeSplitter\n",
        "from stellargraph.mapper import GraphSAGELinkGenerator\n",
        "from stellargraph.mapper import GraphSAGENodeGenerator\n",
        "from stellargraph.layer import GraphSAGE, link_classification\n",
        "from stellargraph.data import UniformRandomWalk\n",
        "from stellargraph.data import BiasedRandomWalk\n",
        "from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
        "from stellargraph.layer import Node2Vec, link_classification\n",
        "from stellargraph import globalvar\n",
        "from stellargraph import StellarGraph\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JloeQrtbW8Fm"
      },
      "source": [
        "#ðŸŒž **2. load the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_sL08dZWt7z"
      },
      "outputs": [],
      "source": [
        "data_path = Drive_path + \"TG_Data_2048_All.pkl\"\n",
        "MyData = pd.read_pickle(data_path)\n",
        "tags_series = MyData['tags']\n",
        "tags_lists = tags_series.str.split(',').apply(lambda tags: [t.strip() for t in tags])\n",
        "all_posts = tags_lists.tolist()\n",
        "print(\"Number of posts : \", len(all_posts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-UxQ4nQXP64"
      },
      "source": [
        "**Select a subset of dataset**\n",
        "\n",
        "if you want to select a subset of data please determine a number, else use 'all'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajoaU2z5QaAY"
      },
      "outputs": [],
      "source": [
        "num_samples = 'all'\n",
        "# num_samples = 1000\n",
        "posts = all_posts[:num_samples] if num_samples != 'all' else  all_posts.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rINkyhPoXFEy"
      },
      "source": [
        "**extract unique tags from the posts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdjah1aE9ces"
      },
      "outputs": [],
      "source": [
        "tags = list(OrderedDict.fromkeys(tag for post in posts for tag in post))\n",
        "print(\"Number of unique tags: \", len(tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqcwAQOAFakD"
      },
      "source": [
        "**statistics about dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SwybcTAFhlg"
      },
      "outputs": [],
      "source": [
        "tag_frequency = Counter(tag for post in posts for tag in post)                  # Step 1: Create a dictionary to store the frequency of each tag\n",
        "tag_frequency_dict = dict(tag_frequency)                                        # Step 2: Count the frequency of each tag across all posts\n",
        "\n",
        "tag_counts = [len(post) for post in posts]                                      # Step 3: Count the number of tags for each post\n",
        "posts_per_tag_count = Counter(tag_counts)                                       # Step 4: Create a dictionary to count the number of posts with a specific number of tags\n",
        "num_tags = list(posts_per_tag_count.keys())\n",
        "num_posts = list(posts_per_tag_count.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WfxcWxX7kpI"
      },
      "source": [
        "#ðŸŒž **3. co-occurrence matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW84BRTkvHLZ"
      },
      "outputs": [],
      "source": [
        "posts_0 = copy.deepcopy(posts)\n",
        "tags_0 = copy.deepcopy(tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux4XH_oghONS"
      },
      "outputs": [],
      "source": [
        "co_occurrence_matrix = np.zeros((len(tags_0), len(tags_0)))                     # Create an empty co-occurrence matrix\n",
        "\n",
        "for post in posts_0:\n",
        "    post_tags = set(post)                                                       # Extract tags from the post\n",
        "    for tag1 in post_tags:                                                      # Iterate through pairs of tags in the post\n",
        "        for tag2 in post_tags:\n",
        "            if tag1 != tag2:\n",
        "                index1 = tags_0.index(tag1)                                     # Find the indices of tag1 and tag2 in the 'tags' list\n",
        "                index2 = tags_0.index(tag2)\n",
        "                co_occurrence_matrix[index1, index2] += 1                       # Update the co-occurrence matrix\n",
        "                co_occurrence_matrix[index2, index1] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNJlqWkhGVMC"
      },
      "outputs": [],
      "source": [
        "display(co_occurrence_matrix[0:10, 0:10])\n",
        "print(co_occurrence_matrix.shape)\n",
        "\n",
        "co_occurrence_df = pd.DataFrame(co_occurrence_matrix)\n",
        "co_occurrence_matrix_file = Drive_path + \"co_occurrence_matrix.pkl\"\n",
        "co_occurrence_df.to_pickle(co_occurrence_matrix_file)\n",
        "\n",
        "print(f\"{co_occurrence_matrix_file} saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyHHpP6KCnzI"
      },
      "source": [
        "**you can load a saved co-occurrence matrix:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-7CcXUO1HVa"
      },
      "outputs": [],
      "source": [
        "co_occurrence_matrix_file = Drive_path + \"co_occurrence_matrix.pkl\"\n",
        "co_occurrence_df = pd.read_pickle(co_occurrence_matrix_file)\n",
        "co_occurrence_matrix = co_occurrence_df.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF0bZga79ZJd"
      },
      "source": [
        "#ðŸŒž **4. graph construction**\n",
        "(NetworkX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY0GVg749rl0"
      },
      "outputs": [],
      "source": [
        "G = nx.Graph()\n",
        "\n",
        "# for tag in tags:                                                                # Add nodes for each tag (including isolate nodes)\n",
        "#     G.add_node(tag)\n",
        "\n",
        "for i in range(len(tags_0)):\n",
        "    for j in range(i + 1, len(tags_0)):\n",
        "        if co_occurrence_matrix[i][j] > 0:\n",
        "            tag1 = tags[i]\n",
        "            tag2 = tags[j]\n",
        "            weight = co_occurrence_matrix[i][j]\n",
        "            G.add_edge(tag1, tag2, weight=weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNj3t24hn3Ri"
      },
      "source": [
        "#ðŸŒž **5. node embedding**\n",
        "(Node2vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1lUgjo8W2YC"
      },
      "source": [
        "**Hyper-parameters**\n",
        "\n",
        "1. `Walk Length:` This parameter controls how long the random walk should be for each node. Longer walks can capture more global structure, but might also introduce noise. Start with a moderate value (e.g., 30) and adjust based on results. *(Range: 10 to 80)*\n",
        "\n",
        "2. `Number of Walks: `The number of random walks to perform from each node. A higher number captures more information but increases computation. Start with a moderate value (e.g., 10) and adjust as needed. *(Range: 5 to 20)*\n",
        "\n",
        "3. `Embedding Dimensions (dimensions):` The size of the embedding vectors. Typically, values between 64 and 128 work well for many tasks. You can experiment with different dimensions to see what works best. *(Range: 64 to 128)*\n",
        "\n",
        "4. `Window Size (window):` The window size during the Skip-gram training. This parameter depends on the size of your graph and the level of granularity you want. Start with a reasonable value (e.g., 10) and adjust based on your results. *(Range: 5 to 20)*\n",
        "\n",
        "5. `min_count:` Ignores all nodes with a total frequency lower than this value. Nodes with a frequency less than min_count are not considered during training. A common starting value is around 5 or 10. You can increase it if you want to filter out rare nodes, or decrease it if you want to include more nodes in the vocabulary.\n",
        "\n",
        "6. `num_workers:` The number of CPU cores to use for training. Setting this to a higher value can speed up training on multi-core machines.\n",
        "\n",
        "7. `sg:` This parameter specifies the training algorithm. Use sg=1 for *Skip-gram* and sg=0 for *CBOW (Continuous Bag of Words)*.\n",
        "\n",
        "8. `p`: The likelihood of backtracking the walk and immediately revisiting a node in the walk is controlled by the return parameter p. Setting a *high value* to parameter p ensures *lower chances of revisiting* a node and *avoids 2-hop redundancy* in sampling. This strategy also encourages moderate graph exploration. On the other hand, if the value of the p parameter is *low*, the chances of backtracking in the walk are *higher*, keeping the random walk *closer to the starting node*.\n",
        "\n",
        "9. `q`: The inOut parameter q allows the traversal calculation to *differentiate between inward and outward nodes*. Setting a *high value* to parameter q (q > 1) biases the random walk to move *towards nodes close to the node in the previous step*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzmJ43KI8mmm"
      },
      "outputs": [],
      "source": [
        "walk_length = 10\n",
        "num_walks = 60\n",
        "dimensions = 128\n",
        "window = 10\n",
        "min_count = 1\n",
        "num_workers = 4\n",
        "sg = 1\n",
        "epochs = 20\n",
        "alpha = 1e-3\n",
        "p = 1\n",
        "q = 0.5\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkU4C24nQIIl"
      },
      "source": [
        "Create a Node2Vec instance, and then, train the Word2Vec model to obtain node embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aovFgMV8LWF"
      },
      "outputs": [],
      "source": [
        "node2vec_instance = Node2Vec(\n",
        "    G,\n",
        "    dimensions=dimensions,\n",
        "    walk_length=walk_length ,\n",
        "    num_walks=num_walks,\n",
        "    workers=num_workers,\n",
        "    p = p,\n",
        "    q = q,\n",
        "    seed = seed,\n",
        "    weight_key = 'weight'\n",
        "  )\n",
        "model = node2vec_instance.fit(\n",
        "    window=window,\n",
        "    min_count=min_count,\n",
        "    sg=sg,\n",
        "    compute_loss=True,\n",
        "    epochs = epochs,\n",
        "    alpha = alpha,\n",
        "    batch_words=4,\n",
        "  )\n",
        "\n",
        "tag_embeddings = {node: model.wv[node] for node in G.nodes()}\n",
        "tag_names = list(tag_embeddings.keys())\n",
        "Node2vec_embeddings = np.array(list(tag_embeddings.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8S-LscPldi"
      },
      "source": [
        "save node embeddings and tag names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUQVmqJblphT"
      },
      "outputs": [],
      "source": [
        "print(len(tag_names))\n",
        "\n",
        "model.wv.save_word2vec_format(f\"{Drive_path}UseRQE/TG/n2v_old_tag_embeddings_(withoutiso)\")\n",
        "model.save(f\"{Drive_path}UseRQE/TG/Node2vec_model\")\n",
        "\n",
        "\n",
        "Node2vec_embeddings_pckl = pd.DataFrame(Node2vec_embeddings)\n",
        "Node2vec_file_name = f\"{Drive_path}UseRQE/TG/n2v_old_embeddings_(withoutiso).pkl\"\n",
        "Node2vec_embeddings_pckl.to_pickle(Node2vec_file_name)\n",
        "\n",
        "\n",
        "tag_names_pckl = pd.DataFrame(tag_names)\n",
        "tag_names_file_name = f\"{Drive_path}UseRQE/TG/n2v_old_tag_names_(withoutiso).pkl\"\n",
        "tag_names_pckl.to_pickle(tag_names_file_name)\n",
        "\n",
        "print(f\"{Node2vec_file_name} saved successfully.\")\n",
        "print(f\"{tag_names_file_name} saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjUlyyXerIHa"
      },
      "source": [
        "**check if embeddings are well-generated**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai_C3WTrP3Pa"
      },
      "outputs": [],
      "source": [
        "edges = list(G.edges())                                                         # Step1: Generate positive and negative samples for training\n",
        "non_edges = [\n",
        "    (i, j)\n",
        "    for i in list(G.nodes())\n",
        "    for j in list(G.nodes())\n",
        "    if not G.has_edge(i, j)\n",
        "]\n",
        "positive_samples = [\n",
        "    (tag_embeddings[i], tag_embeddings[j], 1)\n",
        "    for i, j in edges]\n",
        "negative_samples = [\n",
        "    (tag_embeddings[i], tag_embeddings[j], 0)\n",
        "    for i, j in non_edges]\n",
        "\n",
        "\n",
        "all_samples2 = positive_samples + negative_samples[:296748]                     # Step2: Combine positive and negative samples\n",
        "np.random.shuffle(all_samples2)\n",
        "\n",
        "\n",
        "X = np.array([(np.concatenate((i, j))) for i, j, _ in all_samples2])            # Step3: Split the data into training and testing sets\n",
        "y = np.array([label for _, _, label in all_samples2])\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "classifier = LogisticRegression(max_iter=1000)                                  # Step4: Train a logistic regression classifier for edge prediction\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "predictions = classifier.predict(X_test)                                        # Step5: Make predictions on the test set\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy for edge prediction: {accuracy:.2f}\")\n",
        "print(precision_recall_fscore_support(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8jiGKHe2Sje"
      },
      "outputs": [],
      "source": [
        "Node2vec_file_name = f\"{Drive_path}UseRQE/TG/n2v_old_embeddings_(withoutiso).pkl\"\n",
        "Node2vec_embeddings_pckl = pd.read_pickle(Node2vec_file_name)\n",
        "Node2vec_embeddings = Node2vec_embeddings_pckl.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bLcJvg2ebFx"
      },
      "outputs": [],
      "source": [
        "node_embeddings = Node2vec_embeddings\n",
        "node_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf29MA1iPwNA"
      },
      "source": [
        "#ðŸŒž **6. hierarchical clustering**\n",
        "(sklearn.Agglomerative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPpMyYWbQwRg"
      },
      "source": [
        "**Hyper-parameters**\n",
        "\n",
        "1. `n_clusters`: The number of clusters to form. If None, it will be inferred based on other parameters, such as the dist_threshold. Indeed, the algorithm will continue merging clusters until a certain linkage distance threshold is reached.\n",
        "\n",
        "2. `metric`: The distance metric used to compute the linkage between clusters. This determines how the distance between clusters is measured.Various distance metrics are available, including Euclidean distance *euclidean*, *Manhattan* distance, *cosine* similarity, etc. 'cosine' is a common choice for text and high-dimensional data.\n",
        "\n",
        "3. `dist_threshold`: The linkage distance threshold above which clusters will not be merged. Clusters with distances greater than or equal to this threshold will be treated as separate clusters.This is a float value between 0 and 1.\n",
        "\n",
        "4. `linkage`: The linkage criterion used to compute the distance between newly formed clusters during each merge. It determines how cluster distances are calculated. Various linkage methods are available, including *ward*, *complete*, *average*, *single*, etc. 'average' is a common choice and works well in many cases. 'ward' is another popular option for Euclidean distance-based clustering.\n",
        "\n",
        "5. `full_tree`: If *True*, the algorithm computes the full hierarchy of clusters, allowing you to extract clusters at various distances. If *False*, it stops when n_clusters clusters are reached. Use True when you want to analyze the complete hierarchy of clusters. Use False when you have a specific number of clusters in mind and want the algorithm to stop once that number is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CzUhoIZfYyj"
      },
      "source": [
        "**Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTBb4oVrKrXE"
      },
      "outputs": [],
      "source": [
        "def hierarchical_clustering(embeddings, n_clusters, metric, dist_threshold, linkage, full_tree):\n",
        "    agglomerative_cluster = AgglomerativeClustering(\n",
        "        n_clusters=None,\n",
        "        metric=metric,\n",
        "        distance_threshold=dist_threshold,\n",
        "        linkage=linkage,\n",
        "        compute_full_tree = full_tree\n",
        "    )\n",
        "    pr = agglomerative_cluster.fit_predict(embeddings)\n",
        "    model = agglomerative_cluster.fit(embeddings)\n",
        "    return pr, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F4E8c6dKm6x"
      },
      "outputs": [],
      "source": [
        "def perform_tsne(embeddings, perplexity):                                       # Perform t-SNE for dimensionality reduction\n",
        "    tsne = TSNE(\n",
        "        n_components=2,\n",
        "        random_state=42,\n",
        "        perplexity=perplexity,\n",
        "        n_iter=10000,\n",
        "        learning_rate = 300.0,\n",
        "        n_iter_without_progress = 3000,\n",
        "        early_exaggeration = 50,\n",
        "        method = 'barnes_hut',\n",
        "        # learning_rate_method = None\n",
        "        )\n",
        "    return tsne.fit_transform(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34T_YvYmKhh2"
      },
      "outputs": [],
      "source": [
        "def extract_cluster_statistics(embeddings_2d, cluster_labels, tags, tag_frequency_dict):\n",
        "    data = pd.DataFrame(columns=['X', 'Y', 'Cluster', 'Tag'])                   # Create an empty DataFrame to store data\n",
        "    cluster_means, cluster_variances, cluster_sizes = [], [], []                # Initialize lists to store cluster statistics\n",
        "    representative_samples, representative_samples_freq = {}, {}                # Initialize a dictionary to store the representative sample for each cluster\n",
        "    representative_samples_freq_2nd = {}\n",
        "    representative_samples_freq_3nd = {}\n",
        "\n",
        "    for cluster_label in np.unique(cluster_labels):\n",
        "        samples_tags = [tags[i]\n",
        "                        for i, lbl in enumerate(cluster_labels)\n",
        "                        if lbl == cluster_label]\n",
        "        indexes = [i\n",
        "                   for i, lbl in enumerate(cluster_labels)\n",
        "                   if lbl == cluster_label]\n",
        "        samples_2d = embeddings_2d[cluster_labels == cluster_label]\n",
        "\n",
        "        cluster_center = np.mean(samples_2d, axis=0)\n",
        "        distances = cdist(samples_2d, [cluster_center])\n",
        "        nearest_sample_idx = np.argmin(distances)\n",
        "        representative_samples[cluster_label] = samples_tags[nearest_sample_idx]\n",
        "        print(samples_tags)\n",
        "\n",
        "        representative_samples_freq[cluster_label] = max(\n",
        "            samples_tags,\n",
        "            key=lambda tag: tag_frequency_dict.get(tag, 0))\n",
        "        mostfreq_sample_idx = samples_tags.index(representative_samples_freq[cluster_label])\n",
        "\n",
        "\n",
        "        samples_tags_copy = copy.deepcopy(samples_tags)\n",
        "        samples_tags_copy.remove(representative_samples_freq[cluster_label])\n",
        "        representative_samples_freq_2nd[cluster_label] = max(\n",
        "            samples_tags_copy,\n",
        "            key=lambda tag: tag_frequency_dict.get(tag, 0))\n",
        "        mostfreq_sample_idx_2nd = samples_tags.index(representative_samples_freq_2nd[cluster_label])\n",
        "\n",
        "\n",
        "        samples_tags_copy.remove(representative_samples_freq_2nd[cluster_label])\n",
        "        representative_samples_freq_3nd[cluster_label] = max(\n",
        "            samples_tags_copy,\n",
        "            key=lambda tag: tag_frequency_dict.get(tag, 0))\n",
        "        mostfreq_sample_idx_3nd = samples_tags.index(representative_samples_freq_3nd[cluster_label])\n",
        "\n",
        "\n",
        "\n",
        "        distances_to_representative = cdist(samples_2d, [samples_2d[nearest_sample_idx]])\n",
        "        mean_distance = np.mean(distances_to_representative)\n",
        "        variance_distance = np.var(distances_to_representative)\n",
        "        cluster_size = len(samples_tags)\n",
        "        cluster_means.append(mean_distance)\n",
        "        cluster_variances.append(variance_distance)\n",
        "        cluster_sizes.append(cluster_size)\n",
        "\n",
        "        cluster_data = pd.DataFrame(                                            # Add data for the current cluster to the DataFrame\n",
        "            {\n",
        "                'X': samples_2d[:, 0],\n",
        "                'Y': samples_2d[:, 1],\n",
        "                'Cluster': samples_tags[mostfreq_sample_idx],\n",
        "                'Tag': samples_tags,\n",
        "                'Indexes': indexes\n",
        "            }\n",
        "        )\n",
        "        data = pd.concat([data, cluster_data])\n",
        "\n",
        "\n",
        "    cluster_stats = pd.DataFrame({                                              # Create a new DataFrame for cluster statistics\n",
        "        'Cluster': np.unique(cluster_labels),\n",
        "        'Representative sample': [representative_samples[cluster] for cluster in np.unique(cluster_labels)],\n",
        "        'representative_samples_freq': [representative_samples_freq[cluster] for cluster in np.unique(cluster_labels)],\n",
        "        'representative_samples_freq_2nd': [representative_samples_freq_2nd[cluster] for cluster in np.unique(cluster_labels)],\n",
        "        'representative_samples_freq_3nd': [representative_samples_freq_3nd[cluster] for cluster in np.unique(cluster_labels)],\n",
        "        'Mean_Distance': cluster_means,\n",
        "        'Variance_Distance': cluster_variances,\n",
        "        'Cluster_Size': cluster_sizes,\n",
        "    }).set_index('Cluster', drop=True)\n",
        "\n",
        "\n",
        "    silhouette_metric = silhouette_score(\n",
        "        embeddings_2d,\n",
        "        cluster_labels\n",
        "        )\n",
        "    calinski_harabasz_metric = calinski_harabasz_score(\n",
        "        embeddings_2d,\n",
        "        cluster_labels\n",
        "        )\n",
        "    davies_bouldin_metric = davies_bouldin_score(\n",
        "        embeddings_2d,\n",
        "        cluster_labels\n",
        "        )\n",
        "    return cluster_stats, silhouette_metric, calinski_harabasz_metric, davies_bouldin_metric, data, len(np.unique(cluster_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import to_hex\n",
        "\n",
        "def generate_distinct_colors(n_clusters):                                       # Generate a distinct color palette\n",
        "    cmap = cm.get_cmap('tab20', n_clusters)                                     # 'tab20' or 'tab20c' are good options for distinct colors\n",
        "    return [to_hex(cmap(i)) for i in range(cmap.N)]"
      ],
      "metadata": {
        "id": "V98zbIs1tFqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR4oumC0Kfd0"
      },
      "outputs": [],
      "source": [
        "def visualize_clusters(data, representative_samples, custom_color_scale):       # Create an interactive scatter plot\n",
        "    fig = px.scatter(\n",
        "        data,\n",
        "        x='X',\n",
        "        y='Y',\n",
        "        color= 'Cluster',\n",
        "        hover_data=['Tag'],\n",
        "        labels={'X': 'Dimension 1', 'Y': 'Dimension 2'},\n",
        "        # color_continuous_scale=custom_color_scale\n",
        "        color_discrete_sequence=custom_color_scale,\n",
        "    )\n",
        "    fig.update_traces(\n",
        "        marker=dict(size=2),\n",
        "        selector=dict(mode='markers+text')\n",
        "    )\n",
        "    fig.update_layout(showlegend=False)\n",
        "\n",
        "\n",
        "    # Loop over representative samples and assign unique cluster IDs\n",
        "    for counter, (cluster_label, sample_tag) in enumerate(representative_samples.items()):\n",
        "        representative_sample = data[data['Tag'] == sample_tag]\n",
        "        cluster_color = custom_color_scale[counter % len(custom_color_scale)]\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[representative_sample['X'].values[0]],\n",
        "            y=[representative_sample['Y'].values[0]],\n",
        "            mode=\"markers\",\n",
        "            marker=dict(\n",
        "                size=5,\n",
        "                color='white',\n",
        "                line=dict(width=2, color=cluster_color)\n",
        "            ),\n",
        "            showlegend=False,\n",
        "            hoverinfo=\"text\"\n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        width=1000,\n",
        "        height=800,\n",
        "        plot_bgcolor='rgba(255,255,255,255)'\n",
        "    )\n",
        "    fig.update_xaxes({'gridcolor': 'lightgray', 'zerolinecolor': 'lightgray'})\n",
        "    fig.update_yaxes({'gridcolor': 'lightgray', 'zerolinecolor': 'lightgray'})\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4moEn7GZfQOV"
      },
      "source": [
        "**Dendogram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjSolGu9B4Tv"
      },
      "outputs": [],
      "source": [
        "n_clusters=None\n",
        "metric= 'euclidean'\n",
        "linkage_= 'ward'\n",
        "full_tree = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6aF-L7h2CgA"
      },
      "outputs": [],
      "source": [
        "def plot_dendrogram(model, **kwargs):\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1                                              # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack(\n",
        "        [model.children_, model.distances_, counts]\n",
        "    ).astype(float)\n",
        "\n",
        "    dendrogram(linkage_matrix, **kwargs)                                        # Plot the corresponding dendrogram\n",
        "\n",
        "    return linkage_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, model_Agg = hierarchical_clustering(                                         # Perform hierarchical clustering\n",
        "        embeddings = node_embeddings,\n",
        "        n_clusters = n_clusters,\n",
        "        metric = metric,\n",
        "        dist_threshold = 0,                                                     # setting distance_threshold=0 ensures we compute the full tree.\n",
        "        linkage = linkage_,\n",
        "        full_tree = full_tree\n",
        "    )\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "linkage_matrix = plot_dendrogram(model_Agg)\n",
        "plt.xticks([])\n",
        "plt.xlabel('')\n",
        "plt.savefig(f\"{Drive_path}UseRQE/TG/n2v_old_Dendogram.png\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JlKjNGAH3ir4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axcSYXBcfcnT"
      },
      "source": [
        "Now define thresholds based on the above dendogram"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node_embeddings=np.array(node_embeddings)\n",
        "node_embeddings"
      ],
      "metadata": {
        "id": "YwdLURyG6T-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_rows = 200\n",
        "thresholds = [32, 16, 8]                                                        # Define a list of threshold values for the three levels\n",
        "\n",
        "all_cluster_labels, representative_samples, Whole_data = [], {}, []             # Create a list to store cluster labels at each level\n",
        "                                                                                # and a dictionary to store representative sample names\n",
        "\n",
        "embeddings_2d = perform_tsne(                                                   # Perform t-SNE\n",
        "        embeddings=node_embeddings,\n",
        "        perplexity=60\n",
        "    )\n",
        "\n",
        "\n",
        "for level, threshold in enumerate(thresholds):\n",
        "    print(f\"Threshold: {threshold}\")\n",
        "\n",
        "\n",
        "    cluster_labels = fcluster(                                                  # Cut the dendrogram into clusters at the current threshold\n",
        "        linkage_matrix,\n",
        "        t=threshold,\n",
        "        criterion='distance'\n",
        "        )\n",
        "    all_cluster_labels.append(cluster_labels)\n",
        "\n",
        "    cluster_stats, silhouette_metric,\\\n",
        "     calinski_harabasz_metric, davies_bouldin_metric,\\\n",
        "      data, num_classes = extract_cluster_statistics(                           # Extract cluster statistics\n",
        "        embeddings_2d=embeddings_2d,\n",
        "        cluster_labels=cluster_labels,\n",
        "        tags=list(G.nodes()),\n",
        "        tag_frequency_dict=tag_frequency_dict\n",
        "    )\n",
        "\n",
        "    Whole_data.append(data)\n",
        "    for c_id, rep_name in cluster_stats['representative_samples_freq'].items():       # Store representative sample names with level information\n",
        "        if level == 1:\n",
        "          if any(\n",
        "              value == rep_name\n",
        "              for (key_level, _), value in representative_samples.items()\n",
        "              if key_level == level - 1\n",
        "          ):\n",
        "              representative_samples[(level, c_id)] = cluster_stats['representative_samples_freq_2nd'][c_id]\n",
        "          else:\n",
        "              representative_samples[(level, c_id)] = rep_name\n",
        "        elif level == 2:\n",
        "            if any(\n",
        "                value == rep_name\n",
        "                for (key_level, _), value in representative_samples.items()\n",
        "                if key_level == level - 1\n",
        "            ):\n",
        "                representative_samples[(level, c_id)] = cluster_stats['representative_samples_freq_3nd'][c_id]\n",
        "            elif any(\n",
        "                value == rep_name\n",
        "                for (key_level, _), value in representative_samples.items()\n",
        "                if key_level == level - 2\n",
        "            ):\n",
        "                representative_samples[(level, c_id)] = cluster_stats['representative_samples_freq_3nd'][c_id]\n",
        "            else:\n",
        "                representative_samples[(level, c_id)] = rep_name\n",
        "        else:\n",
        "            representative_samples[(level, c_id)] = rep_name\n",
        "\n",
        "\n",
        "    print(\"Cluster Statistics:\")\n",
        "    display(cluster_stats)\n",
        "    print(\"Silhouette Metric:\", silhouette_metric)\n",
        "    print(\"Calinski Harabasz Metric:\", calinski_harabasz_metric)\n",
        "    print(\"Davies Bouldin Metric:\", davies_bouldin_metric)\n",
        "\n",
        "\n",
        "    # Visualize clusters\n",
        "    # color palette (1)\n",
        "    custom_color_scale = px.colors.sample_colorscale(\"Turbo\", [n/num_classes for n in range(num_classes)])\n",
        "    fig = visualize_clusters(\n",
        "        data=data,\n",
        "        representative_samples=cluster_stats['representative_samples_freq'].to_dict(),\n",
        "        custom_color_scale=custom_color_scale\n",
        "    )\n",
        "    file_path = f\"{Drive_path}UseRQE/TG/n2v_old_clustering_level{level}-a.html\"\n",
        "    pio.write_html(fig, file_path)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "    # color palette (2)\n",
        "    custom_color_scale = px.colors.sequential.Turbo\n",
        "    fig = visualize_clusters(\n",
        "        data=data,\n",
        "        representative_samples=cluster_stats['representative_samples_freq'].to_dict(),\n",
        "        custom_color_scale=custom_color_scale\n",
        "    )\n",
        "    file_path = f\"{Drive_path}UseRQE/TG/n2v_old_clustering_level{level}-b.html\"\n",
        "    pio.write_html(fig, file_path)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "    # color palette (3)\n",
        "    custom_color_scale = generate_distinct_colors(num_classes)\n",
        "    fig = visualize_clusters(\n",
        "        data=data,\n",
        "        representative_samples=cluster_stats['representative_samples_freq'].to_dict(),\n",
        "        custom_color_scale=custom_color_scale\n",
        "    )\n",
        "    file_path = f\"{Drive_path}UseRQE/TG/n2v_old_clustering_level{level}-c.html\"\n",
        "    pio.write_html(fig, file_path)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "0Pt4UnJrJsGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1UjHwMQpRSW"
      },
      "outputs": [],
      "source": [
        "hierarchical_data = []\n",
        "for tag_idx, tag in enumerate(list(G.nodes())):\n",
        "    level_1_cluster = representative_samples.get((0, all_cluster_labels[0][tag_idx]))\n",
        "    level_2_cluster = representative_samples.get((1, all_cluster_labels[1][tag_idx]))\n",
        "    level_3_cluster = representative_samples.get((2, all_cluster_labels[2][tag_idx]))\n",
        "    hierarchical_data.append([level_1_cluster, level_2_cluster, level_3_cluster, tag])\n",
        "\n",
        "print(\"Hierarchical DataFrame:\")\n",
        "hierarchical_df = pd.DataFrame(\n",
        "    hierarchical_data,\n",
        "    columns=[\"Level 1\", \"Level 2\", \"Level 3\", \"Tag\"]\n",
        "    )\n",
        "display(hierarchical_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF2IQQrRc-8c"
      },
      "outputs": [],
      "source": [
        "hierarchical_df_file_name = f\"{Drive_path}UseRQE/TG/n2v_old_hierarchical_df.pkl\"\n",
        "hierarchical_df.to_pickle(hierarchical_df_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjOVWV9T3CNX"
      },
      "outputs": [],
      "source": [
        "hierarchical_df_file_name = f\"{Drive_path}UseRQE/TG/n2v_old_hierarchical_df.pkl\"\n",
        "hierarchical_df = pd.read_pickle(hierarchical_df_file_name)\n",
        "hierarchical_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YZNIz2tgCFE"
      },
      "source": [
        "#ðŸŒž **7. refine the TG-Data**\n",
        "applying hierarchical clustering on the TG data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path_LLama = f\"/content/drive/MyDrive/RQE_Data_With_Both_uesrid.pkl\"\n",
        "MyData_LLama = pd.read_pickle(data_path_LLama)\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='65001'), 'body_Q2']='So when I launch Minecraft, before it finishes loading, it crashes. I do not understand what is going on. Could someone help me? Here is my crash report:'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='36896'), 'body_Q2']='How do I type the infinity symbol in MacTex'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='3031'), 'body_Q2']='Run time error for GP objects'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q1']=='') & (MyData_LLama['userid_Q2']=='65001'), 'body_Q1']='Misplaced allignment tab character line 53'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q1']=='') & (MyData_LLama['userid_Q2']=='16188'), 'body_Q1']='How to Export this animation as a gif file for powerpoint presentation'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q1']=='') & (MyData_LLama['userid_Q2']=='24829'), 'body_Q1']='why does rotation style work on actual coordinates and not variables in tikz 3d plot'\n",
        "\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='50615'), 'body_Q2']='How set a table in margin'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='23835'), 'body_Q2']='Latex equation positioning problem'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='14524'), 'body_Q2']='Chapter comment with regulation'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='50823'), 'body_Q2']='minipage goes beyond right margin'\n",
        "\n",
        "TG_Data = MyData_LLama[['body_Q1', 'tags_Q1']]\n",
        "TG_Data = TG_Data.rename(columns={'body_Q1': 'text', 'tags_Q1': 'tags'})\n",
        "\n",
        "TG_Data2 = pd.read_pickle(f\"{Drive_path}TG_Data (1).pkl\")\n",
        "TG_Data2 = TG_Data2.reset_index(drop=True)\n",
        "TG_Data['tags'] = TG_Data2['tags']\n",
        "display(TG_Data)"
      ],
      "metadata": {
        "id": "hjjKWav6MEGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmoVzBtQ0rfF"
      },
      "outputs": [],
      "source": [
        "tag_mapping = {}                                                                # Step1: Create a mapping dictionary from hierarchical_df\n",
        "for index, row in hierarchical_df.iterrows():\n",
        "    tags = [row['Level 1'], row['Level 2'], row['Level 3']]\n",
        "    tag_mapping[row['Tag']] = ', '.join(tags)\n",
        "\n",
        "result_tags_list = []                                                           # Step2: Process tags in TG_Data\n",
        "oldtags_list = []\n",
        "tags_with_duplicates_list = []\n",
        "\n",
        "for tags_str in TG_Data['tags']:\n",
        "    tags = tags_str.split(', ')\n",
        "    processed_tags = [\n",
        "        tag_mapping.get(tag, tag) if len(tag_mapping.get(tag, tag)) != 0\n",
        "        else tag\n",
        "        for tag in tags\n",
        "    ]\n",
        "    result_tags_list.append(', '.join(processed_tags))\n",
        "    oldtags_list.append(', '.join(tags))\n",
        "    tags_with_duplicates_list.append(', '.join(processed_tags))\n",
        "\n",
        "\n",
        "result_dataframe = pd.DataFrame({                                               # Step3: Create result_dataframe with sorted newtags, tag frequencies, oldtags, and tags_with_duplicates\n",
        "    'text': TG_Data['text'],\n",
        "    'oldtags': oldtags_list,\n",
        "    'newtags': result_tags_list,\n",
        "    'tags_with_duplicates': tags_with_duplicates_list\n",
        "})\n",
        "\n",
        "tag_frequencies_list = [                                                        # Step4: Calculate tag frequencies using Counter\n",
        "    dict(Counter(tags.split(', ')))\n",
        "    for tags in result_tags_list\n",
        "]\n",
        "result_dataframe['tag_frequencies'] = tag_frequencies_list\n",
        "\n",
        "\n",
        "result_dataframe['newtags'] = result_dataframe.apply(                           # Step5: Sort newtags based on frequencies, with the condition to maintain the original order\n",
        "    lambda row: ', '.join(\n",
        "        sorted(\n",
        "            set(row['newtags'].split(', ')),\n",
        "            key=lambda tag: (\n",
        "                row['tag_frequencies'].get(tag, 0),\n",
        "                -row['newtags'].split(', ').index(tag)\n",
        "            ),\n",
        "            reverse=True\n",
        "        )\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "result_dataframe = result_dataframe[[                                           # Step6: Reorder the columns as per the desired output\n",
        "    'text',\n",
        "    'oldtags',\n",
        "    'newtags',\n",
        "    'tags_with_duplicates',\n",
        "    'tag_frequencies'\n",
        "]]\n",
        "TG_Data_After_HieClustering_file_name = f\"{Drive_path}UseRQE/TG/n2v_old_TG_Data_After_HieClustering.pkl\"\n",
        "result_dataframe.to_pickle(TG_Data_After_HieClustering_file_name)\n",
        "display(result_dataframe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wVEOLdZtHa9"
      },
      "source": [
        "if you want to load from a pre-saved TG-Data after hierarchical clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnF34_ve_hc9"
      },
      "outputs": [],
      "source": [
        "TG_Data_After_HieClustering = pd.read_pickle(f\"{Drive_path}UseRQE/TG/n2v_old_TG_Data_After_HieClustering.pkl\")\n",
        "display(TG_Data_After_HieClustering)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
