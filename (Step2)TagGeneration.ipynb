{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SZAftabi/UseRQE/blob/main/(Step2)TagGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfhcCzczV-Gk"
      },
      "source": [
        "<center> <font size='6'> 💟 <b> UseRQE </b> 💟 </font> <br> </center>\n",
        "<center>Recognizing Question Entailment with User Background-knowledge Modeling</center> <center> <font size='4' color='red'> <b> Step (2) </b> Tag generation </font> </center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEdGg56FWsWV"
      },
      "source": [
        "# 😎 **1. mount the drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KphPULSmdv_F",
        "outputId": "0cd5ea89-32db-45c4-f0b4-e35278f53692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiXiD59XAIjV"
      },
      "outputs": [],
      "source": [
        "Drive_path = \"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl4_G7CblsQj"
      },
      "source": [
        "# 😎 **3. Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXoy3AYTBIr3"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers                                                 # ==4.31.0\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q pytorch_lightning\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q -U peft                                                         # ==0.4.0\n",
        "!pip install -q accelerate                                                      # ==0.21.0\n",
        "!pip install -q trl\n",
        "!pip install -q tensorboard\n",
        "!pip install -q datasets\n",
        "!pip install -q rouge\n",
        "!pip install -q bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiX1X7U3USQu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import re\n",
        "import torch\n",
        "import warnings\n",
        "import nltk\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bitsandbytes as bnb\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSacSak3No1M"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade huggingface-hub\n",
        "# !pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyZM5lzmASd0"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import Callback\n",
        "from tensorboard import notebook\n",
        "\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.text.bert import BERTScore\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from torchmetrics.classification import (\n",
        "    BinaryAccuracy,\n",
        "    BinaryPrecision,\n",
        "    BinaryRecall,\n",
        "    BinaryF1Score\n",
        "    )\n",
        "\n",
        "from peft import (\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    AutoPeftModelForCausalLM,\n",
        "    prepare_model_for_kbit_training,\n",
        "    )\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    )\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from nltk.tokenize import word_tokenize\n",
        "from typing import Optional\n",
        "from tqdm import tqdm\n",
        "from bert_score import BERTScorer\n",
        "from rouge import Rouge\n",
        "from statistics import mean\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tqdm.pandas()\n",
        "warnings.filterwarnings('ignore')\n",
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rUff6oUlvrH"
      },
      "source": [
        "# 😎 **4. Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbyTvzmMvGiF"
      },
      "outputs": [],
      "source": [
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "betBMcTQvTkW"
      },
      "outputs": [],
      "source": [
        "def get_tg_prompt(_question, _tags = None):\n",
        "  system_prompt = 'You are a Tag Generator. Respond only with a list of tags; do not include any additional text or explanations.'\n",
        "  user_prompt = f'''Please generate at least 5 tags for the provided question. Tags can include multi-word phrases if appropriate and should help hierarchically categorize the question's topics.\n",
        "### Question:\n",
        "{_question}\n",
        "### Tags:\n",
        "'''\n",
        "  prompt = f\"{B_INST} {B_SYS}{system_prompt}{E_SYS}{user_prompt} {E_INST}\\n\\n\"\n",
        "  if _tags: prompt += f'{_tags}</s>'\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5Ae4n-vgUdR"
      },
      "outputs": [],
      "source": [
        "def get_response_index(_input_ids, _task):\n",
        "  _index = None\n",
        "  _skip_tokens = None\n",
        "  if _task == 'RQE':\n",
        "    _index = 2\n",
        "    _skip_tokens = 10\n",
        "  if _task == 'SUM':\n",
        "    _index = 1\n",
        "    _skip_tokens = 11\n",
        "  if _task == 'TG':\n",
        "    _index = 1\n",
        "    _skip_tokens = 10\n",
        "  hashtags_indexes = [i for i, n in enumerate(_input_ids) if n == 29937]\n",
        "  if len(hashtags_indexes) > _index:\n",
        "    return [i for i, n in enumerate(_input_ids) if n == 29937][_index] + _skip_tokens\n",
        "  elif _task == 'RQE':\n",
        "    return 0\n",
        "  else:\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cIoAmSFvXVq"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data, is_eval):\n",
        "  promp = None\n",
        "  if is_eval: prompt = get_tg_prompt(data['text'])\n",
        "  else: prompt = get_tg_prompt(data['text'], data['tags'])\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FsBMB31t0M8"
      },
      "source": [
        "# 😎 **5. LLama2-TG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDEShe_LPvZu"
      },
      "source": [
        "## 🌻 **5.1. hyper-parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeMlef8LBt4X"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    # ##########################################################################\n",
        "    #                             Configuration\n",
        "    # ##########################################################################\n",
        "    model_name: Optional[str] = field(\n",
        "        default = f\"{Drive_path}llama-2-7b-chat-hf\",\n",
        "        metadata = {\"help\": \"The model that you want to train from the Hugging Face hub.\"}\n",
        "      )\n",
        "    adapter_name: Optional[str] = field(\n",
        "        default = \"LLama-TG\",\n",
        "        metadata = {\"help\": \"The adapter name saved in the HuggingFace hub.\"}\n",
        "      )\n",
        "    save_to: Optional[str] = field(\n",
        "        default = \"Drive\",                                                      # Save to \"Hub\", or \"Drive\", or \"Both\"\n",
        "        metadata = {\"help\": \"Determine where to save Adapters\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                         Logs and Checkpoints\n",
        "    # ##########################################################################\n",
        "    logging_steps: Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"log every X update steps\"}\n",
        "      )\n",
        "    output_dir: Optional[str] = field(\n",
        "        default = \"/content/UseRQE\",\n",
        "        metadata = {\"help\": \"the output directory for both logs and checkpoints\"}\n",
        "      )\n",
        "    every_n_epochs : Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"Save checkpoints every X epochs\"}\n",
        "      )\n",
        "    save_on_train_epoch_end: Optional[bool] = field(\n",
        "        default = None,\n",
        "        metadata = {\"help\": \"Whether to run checkpointing at the end of training epochs or validation\"}\n",
        "      )\n",
        "    total_num_samples: Optional[str] = field(\n",
        "        default = 'All',                                                        # Use {your desired number of samples} or 'All'\n",
        "        metadata = {\"help\": \"Number of samples to be selected from the whole dataset\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                             Hyper-parameters\n",
        "    # ##########################################################################\n",
        "    max_epochs: Optional[int] = field(\n",
        "        default = 10,\n",
        "        metadata = {\"help\": \"maximum number of training epochs.\"}\n",
        "      )\n",
        "    learning_rate: Optional[float] = field(\n",
        "        default = 1e-4,\n",
        "        metadata = {\"help\": \"the learning rate\"}\n",
        "      )\n",
        "    gradient_accumulation_steps: Optional[int] = field(\n",
        "        default = 8,\n",
        "        metadata = {\"help\": \"the number of gradient accumulation steps\"}\n",
        "      )\n",
        "    gradient_checkpointing: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": \"Enables gradient checkpointing.\"}\n",
        "      )\n",
        "    per_device_train_batch_size: Optional[int] = field(\n",
        "        default = 4,\n",
        "        metadata = {\"help\": \"batch_size of training (per device)\"}\n",
        "      )\n",
        "    per_device_eval_batch_size: Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"batch_size of validation (per device)\"}\n",
        "      )\n",
        "    max_seq_length: Optional[int] = field(\n",
        "        default = 512,\n",
        "        metadata = {\"help\": \"maximum input sequence length\"}\n",
        "      )\n",
        "    trust_remote_code: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": '''Enable `trust_remote_code` so that it\n",
        "        will execute code present on the Hub on your local machine'''}\n",
        "      )\n",
        "    split_ratio: Optional[float] = field(\n",
        "        default = (0.8, 0.2, 0),\n",
        "        metadata = {\"help\": \"train/test/validation splits\"}\n",
        "      )\n",
        "    precision: Optional[int] = field(\n",
        "        default = 16,\n",
        "        metadata = {\"help\": \"train with 16/32/bf16 precision.\"}\n",
        "      )\n",
        "    num_sanity_val_steps: Optional[float] = field(\n",
        "        default = 0,\n",
        "        metadata = {\"help\": \"number of validation batches before the first training epoch\"}\n",
        "      )\n",
        "    max_new_tokens: Optional[int] = field(\n",
        "        default = 30,\n",
        "        metadata = {\"help\": \"the maximum number of new tokens in the generated sequences (test step)\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                             Lora Configuration\n",
        "    # ##########################################################################\n",
        "    use_peft: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": \"Wether to use PEFT or not to train adapters\"}\n",
        "      )\n",
        "    lora_r: Optional[int] = field(\n",
        "        default = 64,\n",
        "        metadata = {\"help\": \"the r parameter of the LoRA adapters\"}\n",
        "      )\n",
        "    lora_alpha: Optional[int] = field(\n",
        "        default = 64,\n",
        "        metadata = {\"help\": \"the alpha parameter of the LoRA adapters\"}\n",
        "      )\n",
        "    lora_dropout: Optional[int] = field(\n",
        "        default = 0.1,\n",
        "        metadata = {\"help\": \"the dropout rate of the LoRA adapters\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                                 BitsAndBytes\n",
        "    # ##########################################################################\n",
        "    load_in_8bit: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"load the model in 8 bits precision\"}\n",
        "      )\n",
        "    load_in_4bit: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"load the model in 4 bits precision\"}\n",
        "      )\n",
        "    use_nested_quant: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"Activate nested quantization for 4bit base models\"}\n",
        "      )\n",
        "    bnb_4bit_compute_dtype: Optional[str] = field(\n",
        "        default = \"float16\",\n",
        "        metadata = {\"help\": \"Compute dtype for 4bit base models\"}\n",
        "      )\n",
        "    bnb_4bit_quant_type: Optional[str] = field(\n",
        "        default = \"nf4\",\n",
        "        metadata = {\"help\": \"Quantization type fp4 or nf4\"}\n",
        "      )\n",
        "\n",
        "parser = HfArgumentParser(ScriptArguments)\n",
        "script_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y2OpIGibCoy"
      },
      "source": [
        "## 🌻 **5.2. proposed model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMU0d3K93G5i"
      },
      "outputs": [],
      "source": [
        "class OverrideEpochStepCallback(Callback):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_test_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def _log_step_as_current_epoch(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        pl_module.log(\"step\", trainer.current_epoch + 1)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(every_n_epochs=script_args.every_n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiM-793khoXQ"
      },
      "outputs": [],
      "source": [
        "class TGModel(pl.LightningModule):\n",
        "    def __init__(self, script_args):\n",
        "        super(TGModel, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.Setup(script_args)\n",
        "        self.rouge = ROUGEScore()\n",
        "        self.adapter_name = script_args.adapter_name\n",
        "        self.epoch_n = 1\n",
        "\n",
        "    def Setup(self, script_args):\n",
        "        if script_args.load_in_4bit and script_args.load_in_8bit:\n",
        "          raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
        "        elif script_args.load_in_4bit:\n",
        "          compute_dtype = getattr(torch, script_args.bnb_4bit_compute_dtype)\n",
        "\n",
        "          bnb_config = BitsAndBytesConfig(\n",
        "              load_in_4bit = script_args.load_in_4bit,\n",
        "              bnb_4bit_quant_type = script_args.bnb_4bit_quant_type,\n",
        "              bnb_4bit_compute_dtype = compute_dtype,\n",
        "              bnb_4bit_use_double_quant = script_args.use_nested_quant,\n",
        "          )\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              quantization_config = bnb_config,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "        elif script_args.load_in_8bit:\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              load_in_8bit = True,\n",
        "              torch_dtype = torch.float16,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "          self.model = prepare_model_for_kbit_training(self.model)\n",
        "        else:\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              torch_dtype = torch.bfloat16,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "\n",
        "        if script_args.use_peft:\n",
        "            lora_config = LoraConfig(\n",
        "                task_type = TaskType.CAUSAL_LM,\n",
        "                r = script_args.lora_r,\n",
        "                lora_alpha = script_args.lora_alpha,\n",
        "                lora_dropout = script_args.lora_dropout,\n",
        "                bias = \"none\",\n",
        "            )\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "        self.model.config.use_cache = False\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            script_args.model_name,\n",
        "            padding_side='left'\n",
        "        )\n",
        "        self.tokenizer.pad_token_id = 0\n",
        "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        output = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "            )\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        loss, _ = self.forward(input_ids, attention_mask, labels)\n",
        "        self.log('train_loss', loss.item(), on_epoch=True, on_step=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "      out_dir = f\"{Drive_path}/UseRQE/TG/TG-Adapters/\"\n",
        "      self.model.save_pretrained(out_dir + self.adapter_name + str(self.epoch_n))\n",
        "      self.epoch_n += 1\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "      return self.model.generate(*args, **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.model.parameters(), lr=script_args.learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m4HseXz2F8e"
      },
      "source": [
        "## 🌻 **5.3. model compile**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ARauFuv2lrq"
      },
      "outputs": [],
      "source": [
        "MyModel = TGModel(script_args)\n",
        "logger = TensorBoardLogger(script_args.output_dir + 'logs', name=\"TG\")\n",
        "\n",
        "print(MyModel)\n",
        "print(\"#\"*60, \"\\n\\t\\t\\t Model Configuration\\n\", \"#\"*60)\n",
        "print(MyModel.model.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7REjn7V2KN7"
      },
      "source": [
        "## 🌻 **5.4. data preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGdsYXuAq4br"
      },
      "outputs": [],
      "source": [
        "MyData = pd.read_pickle(f\"{Drive_path}UseRQE/TG/n2v_old_TG_Data_After_HieClustering.pkl\")\n",
        "MyData = MyData[['text', 'newtags']]\n",
        "MyData.rename(columns = {'newtags': 'tags'}, inplace = True)\n",
        "\n",
        "if script_args.total_num_samples != 'All':\n",
        "  MyData = MyData[:int(script_args.total_num_samples)]\n",
        "\n",
        "print(MyData.shape)\n",
        "display(MyData[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zvhk7mmiFfo"
      },
      "outputs": [],
      "source": [
        "class TGDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len, is_eval):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.is_eval = is_eval\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      row_data = self.data.iloc[index]\n",
        "      prompt = generate_prompt(row_data, self.is_eval)\n",
        "      prompt_encoding = self.tokenizer(\n",
        "          prompt,\n",
        "          max_length = self.max_len,\n",
        "          padding = 'max_length',\n",
        "          truncation = True,\n",
        "          add_special_tokens = True,\n",
        "          return_tensors = 'pt',\n",
        "      )\n",
        "      input_ids = prompt_encoding['input_ids'].squeeze()\n",
        "      attention_mask = prompt_encoding['attention_mask'].squeeze()\n",
        "\n",
        "      if self.is_eval == False:\n",
        "        response_index = get_response_index(input_ids, 'TG')\n",
        "        if response_index:\n",
        "          labels = torch.cat((torch.full((response_index,), -100), input_ids[response_index:])).squeeze()\n",
        "        else:\n",
        "          print('response_index not found')\n",
        "      else:\n",
        "        labels = self.tokenizer(\n",
        "            row_data['tags'] + '</s>',\n",
        "            add_special_tokens = False,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        labels = labels['input_ids'].squeeze()\n",
        "      return {\n",
        "          'input_ids': input_ids,\n",
        "          'attention_mask': attention_mask,\n",
        "          'labels': labels\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h-Li53GiLvZ"
      },
      "outputs": [],
      "source": [
        "class TGDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data, tokenizer, script_args):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.per_device_train_batch_size = script_args.per_device_train_batch_size\n",
        "        self.per_device_eval_batch_size = script_args.per_device_eval_batch_size\n",
        "        self.max_len = script_args.max_seq_length\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        len_tr = int(script_args.split_ratio[0] * self.data.shape[0])\n",
        "        len_te = int(script_args.split_ratio[1] * self.data.shape[0])\n",
        "        train_data, test_data = train_test_split(self.data,\n",
        "                                                 test_size=len_te,\n",
        "                                                 random_state=42)\n",
        "        train_data.reset_index(drop=True, inplace=True)\n",
        "        test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        self.train_data = TGDataset(train_data, self.tokenizer, self.max_len, is_eval=False)\n",
        "        self.test_data = TGDataset(test_data, self.tokenizer, self.max_len, is_eval=True)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=self.per_device_train_batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=8,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.test_data,\n",
        "            sampler = torch.utils.data.SequentialSampler(self.test_data,),\n",
        "            batch_size= self.per_device_eval_batch_size,\n",
        "            num_workers=8\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEouim7oiWKm"
      },
      "outputs": [],
      "source": [
        "DataModule = TGDataModule(\n",
        "    MyData,\n",
        "    MyModel.tokenizer,\n",
        "    script_args\n",
        ")\n",
        "print(\"num train batches\", len(DataModule.train_dataloader()))\n",
        "print(\"num test batches\", len(DataModule.test_dataloader()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQVIZYtk7d8j"
      },
      "source": [
        "## 🌻 **5.5. training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi_Ws16M241J"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    logger = logger,\n",
        "    log_every_n_steps = script_args.logging_steps,\n",
        "    max_epochs = script_args.max_epochs,\n",
        "    accumulate_grad_batches = script_args.gradient_accumulation_steps,\n",
        "    num_sanity_val_steps = script_args.num_sanity_val_steps,\n",
        "    callbacks = [OverrideEpochStepCallback(), checkpoint_callback],\n",
        "    default_root_dir= script_args.output_dir + 'Checkpoints',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0FOy8lOVsDj"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/UseRQElogs\n",
        "\n",
        "trainer.fit(\n",
        "    MyModel,\n",
        "    datamodule=DataModule,\n",
        ")\n",
        "\n",
        "!cp -r /content/UseRQElogs /content/drive/MyDrive/UseRQE/TG/UseRQElogs_TG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGfeKtJfWWTd"
      },
      "source": [
        "## 🌻 **5.6. save adapters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fp2pRyKyGBF"
      },
      "source": [
        "save model in:<br>\n",
        "1.    **local directory** 📁   \n",
        " or   <br>\n",
        "2.   **HuggingFace 🤗 Hub**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBOj6BwYWcC4"
      },
      "outputs": [],
      "source": [
        "if script_args.save_to == \"Both\" or script_args.save_to == \"Drive\":\n",
        "  MyModel.model.save_pretrained(\n",
        "      f\"{Drive_path}UseRQE/TG/TG-Adapters/{script_args.adapter_name}\"\n",
        "      )\n",
        "  print(\n",
        "      \"Model successfully saved in \",\n",
        "      script_args.output_dir + script_args.adapter_name\n",
        "      )\n",
        "\n",
        "if script_args.save_to == \"Both\" or script_args.save_to == \"Hub\":\n",
        "  MyModel.model.push_to_hub(script_args.adapter_name)\n",
        "  print(\"Model successfully saved in \", script_args.adapter_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MyModel.model.save_pretrained(f\"/content/drive/MyDrive/UseRQE/TG/TG-Adapters/LLama-TG10\")"
      ],
      "metadata": {
        "id": "Q3KhKtYjIhQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBnjaoEvKMhw"
      },
      "source": [
        "## 🌻 **5.7. load model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhkmyo_xJESu"
      },
      "outputs": [],
      "source": [
        "del tokenizer\n",
        "del trainer\n",
        "del MyModel\n",
        "del fModel\n",
        "del BaseModel\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy3--dnj1-HJ"
      },
      "outputs": [],
      "source": [
        "BaseModel= AutoModelForCausalLM.from_pretrained(\n",
        "    f\"{Drive_path}llama-2-7b-chat-hf\",\n",
        "    device_map={\"\": 0},\n",
        "    offload_folder=\"offload\",\n",
        "    offload_state_dict = True,\n",
        "    # load_in_8bit = True,\n",
        "    )\n",
        "\n",
        "address = f\"/content/drive/MyDrive/UseRQE/TG/TG-Adapters/LLama-TG10\"\n",
        "print(\"\\n Loading model from \", address, \"\\n\")\n",
        "config = PeftConfig.from_pretrained(address)\n",
        "fModel= PeftModel.from_pretrained(BaseModel,address,device_map={\"\": 0})\n",
        "fModel = fModel.merge_and_unload()\n",
        "print(fModel)\n",
        "print(fModel.config)\n",
        "print(\"\\n Model successfully loded from \", address, \"\\n\")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    script_args.model_name,\n",
        "    padding_side='left'\n",
        "    )\n",
        "\n",
        "tokenizer.pad_token_id = 0\n",
        "fModel.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT1Gsgz1wMxO"
      },
      "outputs": [],
      "source": [
        "DataModule = TGDataModule(\n",
        "    MyData,\n",
        "    tokenizer,\n",
        "    script_args\n",
        ")\n",
        "print(\"num train batches\", len(DataModule.train_dataloader()))\n",
        "print(\"num test batches\", len(DataModule.test_dataloader()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5_azJeDaElC"
      },
      "source": [
        "## 🌻 **5.8. test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-c54lijLeBL"
      },
      "outputs": [],
      "source": [
        "def test_step(test_dl):\n",
        "  testOutputs = []\n",
        "\n",
        "  for batch in test_dl:\n",
        "    input_ids = batch['input_ids'].cuda()\n",
        "    attention_mask = batch['attention_mask'].cuda()\n",
        "    labels = batch['labels'].cuda()\n",
        "\n",
        "    generated_txts_ids = fModel.generate(\n",
        "        input_ids = input_ids,\n",
        "        max_new_tokens = script_args.max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.97\n",
        "        ).squeeze()\n",
        "\n",
        "    generated_txts = tokenizer.decode(\n",
        "        generated_txts_ids[get_response_index(generated_txts_ids, 'TG'):],\n",
        "        skip_special_tokens = False,\n",
        "        clean_up_tokenization_spaces = True\n",
        "        )\n",
        "\n",
        "    labels = torch.where(\n",
        "        labels != -100,\n",
        "        labels,\n",
        "        tokenizer.pad_token_id\n",
        "        ).squeeze()\n",
        "\n",
        "    target_txts = tokenizer.decode(\n",
        "        labels,\n",
        "        skip_special_tokens = False,\n",
        "        clean_up_tokenization_spaces = True\n",
        "        )\n",
        "\n",
        "    testOutputs.append([generated_txts[:-4], target_txts[:-4]])\n",
        "\n",
        "  return testOutputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su5vnMd1Nzai"
      },
      "outputs": [],
      "source": [
        "fModel.eval()\n",
        "testOutputs = test_step(DataModule.test_dataloader())\n",
        "testOutputs_file_name = f\"{Drive_path}UseRQE/TG/TG_test_outputs.pkl\"\n",
        "pd.DataFrame(testOutputs).to_pickle(testOutputs_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZdUtlkIZ341"
      },
      "outputs": [],
      "source": [
        "testOutputs_file_name = f\"{Drive_path}UseRQE/TG/TG_test_outputs.pkl\"\n",
        "testOutputs = pd.read_pickle(testOutputs_file_name)\n",
        "testOutputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testOutputs.columns=['generated_tags', 'target_tags']\n",
        "testOutputs['generated_tags'] = testOutputs['generated_tags'].str.replace('\\n\\n', '')\n",
        "testOutputs\n",
        "\n",
        "substring = '[/INST]'\n",
        "testOutputs['generated_tags'] = testOutputs['generated_tags'].apply(\n",
        "    lambda x: x[x.find(substring)+7:] if substring in x else \"\"\n",
        ")\n",
        "display(testOutputs)"
      ],
      "metadata": {
        "id": "5SPtIJgs5_kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW6Z0e5PPiol"
      },
      "source": [
        "## 🌻 **5.9. test evaluation**\n",
        "Rouge & BERTScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBjtKJnnyAvt"
      },
      "outputs": [],
      "source": [
        "testOutputs_file_name = f\"{Drive_path}UseRQE/TG/TG_test_outputs.pkl\"\n",
        "testOutputs = pd.read_pickle(testOutputs_file_name)\n",
        "testOutputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuY0CkbRPqgf"
      },
      "outputs": [],
      "source": [
        "scorer = BERTScorer(lang=\"en\", device=\"cuda\")\n",
        "P, R, F1 = scorer.score(testOutputs['generated_tags'].to_list(), testOutputs['target_tags'].to_list(), verbose=False)\n",
        "print(f\"BERTScore Precision: {P}\")\n",
        "print(f\"BERTScore Recall: {R}\")\n",
        "print(f\"BERTScore F1: {F1}\")\n",
        "print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")\n",
        "\n",
        "rouge = Rouge()\n",
        "scores2 = rouge.get_scores(testOutputs['generated_tags'].to_list(), testOutputs['target_tags'].to_list(), avg=True)\n",
        "print(\"rouge-1:\", scores2['rouge-1'])\n",
        "print(\"rouge-2:\",scores2['rouge-2'])\n",
        "print(\"rouge-l:\",scores2['rouge-l'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}