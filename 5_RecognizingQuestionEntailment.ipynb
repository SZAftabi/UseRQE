{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SZAftabi/UseRQE/blob/main/5_RecognizingQuestionEntailment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfhcCzczV-Gk"
      },
      "source": [
        "<center> <font size='6'> üíü <b> UseRQE </b> üíü </font> <br> </center>\n",
        "<center>Recognizing Question Entailment with User Background-knowledge Modeling <br> </center> <center> <font size='4' color='red'> <b> Step (5) </b> User-aware/User-agnostic question entailment recognition </font> </center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEdGg56FWsWV"
      },
      "source": [
        "# üòé **Mount the drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KphPULSmdv_F"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiXiD59XAIjV"
      },
      "outputs": [],
      "source": [
        "Drive_path = \"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl4_G7CblsQj"
      },
      "source": [
        "# üòé **1. Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXoy3AYTBIr3"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers                                                 # ==4.31.0\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q pytorch_lightning\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q -U peft                                                         # ==0.4.0\n",
        "!pip install -q accelerate                                                      # ==0.21.0\n",
        "!pip install -q trl\n",
        "!pip install -q tensorboard\n",
        "!pip install -q datasets\n",
        "!pip install -q rouge\n",
        "!pip install -q bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiX1X7U3USQu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import warnings\n",
        "import nltk\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import sklearn\n",
        "import gc\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bitsandbytes as bnb\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSacSak3No1M"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade -q huggingface-hub\n",
        "# !pip install --upgrade -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyZM5lzmASd0"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning import Callback\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from tensorboard import notebook\n",
        "\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.text.bert import BERTScore\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from torchmetrics.classification import (\n",
        "    BinaryAccuracy,\n",
        "    BinaryPrecision,\n",
        "    BinaryRecall,\n",
        "    BinaryF1Score\n",
        "    )\n",
        "\n",
        "from peft import (\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    AutoPeftModelForCausalLM,\n",
        "    prepare_model_for_kbit_training,\n",
        "    )\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    )\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from nltk.tokenize import word_tokenize\n",
        "from typing import Optional\n",
        "from tqdm import tqdm\n",
        "from bert_score import BERTScorer\n",
        "from rouge import Rouge\n",
        "from statistics import mean\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "\n",
        "tqdm.pandas()\n",
        "warnings.filterwarnings('ignore')\n",
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rUff6oUlvrH"
      },
      "source": [
        "# üòé **2. Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbyTvzmMvGiF"
      },
      "outputs": [],
      "source": [
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtAPX9bMvNMt"
      },
      "outputs": [],
      "source": [
        "# ========== User-agnostic ==========\n",
        "# def get_rqe_prompt(_q1, _q2, _entailment=None):\n",
        "#     system_prompt = \"Given two questions, Q1 and Q2, determine if Q1 entails Q2 or not.\"\n",
        "#     user_prompt = f'''Entailment means every answer to Q2 must fully or partially answer Q1.\n",
        "# Respond with \"Entailed\" or \"Not-entailed\" only.\n",
        "# Q1: {_q1}\n",
        "# Q2: {_q2}\n",
        "# ### Answer:\n",
        "# '''\n",
        "#     prompt = f\"{B_INST} {B_SYS}{system_prompt}{E_SYS}{user_prompt}{E_INST}\\n\\n\"\n",
        "#     if _entailment: prompt += f\"{_entailment}</s>\"\n",
        "#     return prompt\n",
        "\n",
        "\n",
        "# ========== User-aware ==========\n",
        "def get_rqe_prompt(_q1, _q2, _BN, _entailment=None):\n",
        "    system_prompt = \"Given two questions, Q1 and Q2, determine if Q1 entails Q2 or not.\"\n",
        "    user_prompt = f'''Entailment means every answer to Q2 must fully or partially answer Q1.\n",
        "Note that, Q2 must align with the user's topics of interest: ({_BN}).\n",
        "Respond with \"Entailed\" or \"Not-entailed\" only.\n",
        "Q1: {_q1}\n",
        "Q2: {_q2}\n",
        "### Answer:\n",
        "'''\n",
        "    prompt = f\"{B_INST} {B_SYS}{system_prompt}{E_SYS}{user_prompt}{E_INST}\\n\\n\"\n",
        "    if _entailment: prompt += f\"{_entailment}</s>\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5Ae4n-vgUdR"
      },
      "outputs": [],
      "source": [
        "def get_response_index(_input_ids, _task):\n",
        "  _index = None\n",
        "  _skip_tokens = None\n",
        "  if _task == 'RQE':\n",
        "    _index = 0\n",
        "    _skip_tokens = 10\n",
        "  if _task == 'SUM':\n",
        "    _index = 1\n",
        "    _skip_tokens = 11\n",
        "  if _task == 'TG':\n",
        "    _index = 1\n",
        "    _skip_tokens = 10\n",
        "  hashtags_indexes = [i for i, n in enumerate(_input_ids) if n == 29937]\n",
        "  if len(hashtags_indexes) > _index:\n",
        "    return [i for i, n in enumerate(_input_ids) if n == 29937][_index] + _skip_tokens\n",
        "  elif _task == 'RQE':\n",
        "    return 0\n",
        "  else:\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMhd2UoVLocJ"
      },
      "outputs": [],
      "source": [
        "# ========== User-agnostic ==========\n",
        "# def generate_prompt_rqe(data, is_eval):\n",
        "#   promp = None\n",
        "#   if is_eval: prompt = get_rqe_prompt(data['q1'], data['q2'])\n",
        "#   else: prompt = get_rqe_prompt(data['q1'], data['q2'], data['entailment'])\n",
        "#   return prompt\n",
        "\n",
        "\n",
        "# ========== User-aware ==========\n",
        "def generate_prompt_rqe(data, is_eval):\n",
        "  promp = None\n",
        "  if is_eval: prompt = get_rqe_prompt(data['q1'], data['q2'], data['U_Background_kn'])\n",
        "  else: prompt = get_rqe_prompt(data['q1'], data['q2'], data['U_Background_kn'], data['entailment'])\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcAIhXF85GNH"
      },
      "source": [
        "# üòé **3. LLama2-RQE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTHxL5745EFy"
      },
      "source": [
        "## üåª **3.1. hyper-parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIiJpbnO5EF-"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    # ##########################################################################\n",
        "    #                             Configuration\n",
        "    # ##########################################################################\n",
        "    model_name: Optional[str] = field(\n",
        "        default = f\"{Drive_path}llama-2-7b-chat-hf\",\n",
        "        metadata = {\"help\": \"The model that you want to train from the Hugging Face hub.\"}\n",
        "      )\n",
        "    adapter_name: Optional[str] = field(\n",
        "        default = \"LLama-RQE\",\n",
        "        metadata = {\"help\": \"The adapter name saved in the HuggingFace hub.\"}\n",
        "      )\n",
        "    save_to: Optional[str] = field(\n",
        "        default = \"Drive\",                                                       # Save to \"Hub\", or \"Drive\", or \"Both\"\n",
        "        metadata = {\"help\": \"Determine where to save Adapters\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                         Logs and Checkpoints\n",
        "    # ##########################################################################\n",
        "    logging_steps: Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"log every X update steps\"}\n",
        "      )\n",
        "    output_dir: Optional[str] = field(\n",
        "        default = \"/content/LLama\",\n",
        "        metadata = {\"help\": \"the output directory\"}\n",
        "      )\n",
        "    every_n_epochs : Optional[int] = field(\n",
        "        default = 1,\n",
        "        metadata = {\"help\": \"Save checkpoints every X epochs\"}\n",
        "      )\n",
        "    save_on_train_epoch_end: Optional[bool] = field(\n",
        "        default = None,\n",
        "        metadata = {\"help\": \"Whether to run checkpointing at the end of training epochs or validation\"}\n",
        "      )\n",
        "    total_num_samples: Optional[str] = field(\n",
        "        default = 'All',\n",
        "        metadata = {\"help\": \"Number of samples to be selected from the whole dataset\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                             Hyper-parameters\n",
        "    # ##########################################################################\n",
        "    max_epochs: Optional[int] = field(\n",
        "        default = 10,\n",
        "        metadata = {\"help\": \"maximum number of training epochs.\"}\n",
        "      )\n",
        "    learning_rate: Optional[float] = field(\n",
        "        default = 1e-4,\n",
        "        metadata = {\"help\": \"the learning rate\"}\n",
        "      )\n",
        "    gradient_accumulation_steps: Optional[int] = field(\n",
        "        default = 2,\n",
        "        metadata = {\"help\": \"the number of gradient accumulation steps\"}\n",
        "      )\n",
        "    gradient_checkpointing: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": \"Enables gradient checkpointing.\"}\n",
        "      )\n",
        "    per_device_train_batch_size: Optional[int] = field(\n",
        "        default = 8,\n",
        "        metadata = {\"help\": \"batch_size of training (per device)\"}\n",
        "      )\n",
        "    per_device_eval_batch_size: Optional[int] = field(\n",
        "        default = 8,\n",
        "        metadata = {\"help\": \"batch_size of validation (per device)\"}\n",
        "      )\n",
        "    max_seq_length: Optional[int] = field(\n",
        "        default = 512,\n",
        "        metadata = {\"help\": \"maximum input sequence length\"}\n",
        "      )\n",
        "    trust_remote_code: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": '''Enable `trust_remote_code` so that it\n",
        "        will execute code present on the Hub on your local machine'''}\n",
        "      )\n",
        "    split_ratio: Optional[float] = field(\n",
        "        default = (0.8, 0.2, 0),\n",
        "        metadata = {\"help\": \"train/test/validation splits\"}\n",
        "      )\n",
        "    precision: Optional[int] = field(\n",
        "        default = 16,\n",
        "        metadata = {\"help\": \"train with 16/32/bf16 precision.\"}\n",
        "      )\n",
        "    num_sanity_val_steps: Optional[float] = field(\n",
        "        default = 0,\n",
        "        metadata = {\"help\": \"number of validation batches before the first training epoch\"}\n",
        "      )\n",
        "    max_new_tokens: Optional[int] = field(\n",
        "        default = 5,\n",
        "        metadata = {\"help\": \"the maximum number of new tokens in the generated sequences (test step)\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                             Lora Configuration\n",
        "    # ##########################################################################\n",
        "    use_peft: Optional[bool] = field(\n",
        "        default = True,\n",
        "        metadata = {\"help\": \"Wether to use PEFT or not to train adapters\"}\n",
        "      )\n",
        "    lora_r: Optional[int] = field(\n",
        "        default = 64,\n",
        "        metadata = {\"help\": \"the r parameter of the LoRA adapters\"}\n",
        "      )\n",
        "    lora_alpha: Optional[int] = field(\n",
        "        default = 64,\n",
        "        metadata = {\"help\": \"the alpha parameter of the LoRA adapters\"}\n",
        "      )\n",
        "    lora_dropout: Optional[int] = field(\n",
        "        default = 0.1,\n",
        "        metadata = {\"help\": \"the dropout rate of the LoRA adapters\"}\n",
        "      )\n",
        "    # ##########################################################################\n",
        "    #                                 BitsAndBytes\n",
        "    # ##########################################################################\n",
        "    load_in_8bit: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"load the model in 8 bits precision\"}\n",
        "      )\n",
        "    load_in_4bit: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"load the model in 4 bits precision\"}\n",
        "      )\n",
        "    use_nested_quant: Optional[bool] = field(\n",
        "        default = False,\n",
        "        metadata = {\"help\": \"Activate nested quantization for 4bit base models\"}\n",
        "      )\n",
        "    bnb_4bit_compute_dtype: Optional[str] = field(\n",
        "        default = \"float16\",\n",
        "        metadata = {\"help\": \"Compute dtype for 4bit base models\"}\n",
        "      )\n",
        "    bnb_4bit_quant_type: Optional[str] = field(\n",
        "        default = \"nf4\",\n",
        "        metadata = {\"help\": \"Quantization type fp4 or nf4\"}\n",
        "      )\n",
        "\n",
        "parser = HfArgumentParser(ScriptArguments)\n",
        "script_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtYfn89p5EF_"
      },
      "source": [
        "## üåª **3.2. proposed model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBKwuzno5EGA"
      },
      "outputs": [],
      "source": [
        "class OverrideEpochStepCallback(Callback):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_test_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        self._log_step_as_current_epoch(trainer, pl_module)\n",
        "\n",
        "    def _log_step_as_current_epoch(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        pl_module.log(\"step\", trainer.current_epoch + 1)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(every_n_epochs = script_args.every_n_epochs,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIAxFSrq5EGB"
      },
      "outputs": [],
      "source": [
        "class RQEModel(pl.LightningModule):\n",
        "    def __init__(self, script_args):\n",
        "        super(RQEModel, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.Setup(script_args)\n",
        "        self.rouge = ROUGEScore()\n",
        "        self.adapter_name = script_args.adapter_name\n",
        "        self.epoch_n = 1\n",
        "\n",
        "    def Setup(self, script_args):\n",
        "        if script_args.load_in_4bit and script_args.load_in_8bit:\n",
        "          raise ValueError(\n",
        "              \"You can't load the model in 8 bits and 4 bits at the same time\"\n",
        "              )\n",
        "        elif script_args.load_in_4bit:\n",
        "          compute_dtype = getattr(torch, script_args.bnb_4bit_compute_dtype)\n",
        "\n",
        "          bnb_config = BitsAndBytesConfig(\n",
        "              load_in_4bit = script_args.load_in_4bit,\n",
        "              bnb_4bit_quant_type = script_args.bnb_4bit_quant_type,\n",
        "              bnb_4bit_compute_dtype = compute_dtype,\n",
        "              bnb_4bit_use_double_quant = script_args.use_nested_quant,\n",
        "          )\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              quantization_config = bnb_config,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "        elif script_args.load_in_8bit:\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              load_in_8bit = True,\n",
        "              torch_dtype = torch.float16,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "          self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "        else:\n",
        "          self.model = AutoModelForCausalLM.from_pretrained(\n",
        "              script_args.model_name,\n",
        "              torch_dtype = torch.bfloat16,\n",
        "              device_map = {\"\": 0},\n",
        "          )\n",
        "\n",
        "        if script_args.use_peft:\n",
        "            lora_config = LoraConfig(\n",
        "                task_type = TaskType.CAUSAL_LM,\n",
        "                r = script_args.lora_r,\n",
        "                lora_alpha = script_args.lora_alpha,\n",
        "                lora_dropout = script_args.lora_dropout,\n",
        "                bias = \"none\",\n",
        "            )\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "        self.model.config.use_cache = False\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            script_args.model_name,\n",
        "            padding_side='left'\n",
        "        )\n",
        "        self.tokenizer.pad_token_id = 0\n",
        "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        output = self.model(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            labels=labels\n",
        "                            )\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        loss, _ = self.forward(input_ids, attention_mask, labels)\n",
        "        self.log('train_loss', loss.item(), on_epoch=True, on_step=True)\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "\n",
        "      # ========== User-aware ==========\n",
        "      out_dir = f\"{Drive_path}/LLama/LLAMA-RQE-UM/\"\n",
        "\n",
        "      # ========== User-agnostic ==========\n",
        "      # out_dir = f\"{Drive_path}/LLama/LLAMA-RQE-WoUM/\"\n",
        "\n",
        "      self.model.save_pretrained(\n",
        "          out_dir + self.adapter_name + str(self.epoch_n)\n",
        "          )\n",
        "      self.epoch_n += 1\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "      return self.model.generate(*args, **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=script_args.learning_rate\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOyj6Wjq5EGB"
      },
      "source": [
        "## üåª **3.3. model compile**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ai231d85EGC"
      },
      "outputs": [],
      "source": [
        "MyModel = RQEModel(script_args)\n",
        "logger = TensorBoardLogger(script_args.output_dir + 'logs', name=\"RQE\")\n",
        "\n",
        "print(MyModel)\n",
        "print(\"#\"*60, \"\\n\\t\\t\\t Model Configuration\\n\", \"#\"*60)\n",
        "print(MyModel.model.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feEgaoeD5EGC"
      },
      "source": [
        "## üåª **3.4. data preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_Cn3Ie866tC"
      },
      "outputs": [],
      "source": [
        "data_path_LLama = f\"/content/drive/MyDrive/RQE_Data.pkl\"\n",
        "MyData_LLama = pd.read_pickle(data_path_LLama)\n",
        "\n",
        "MyData2 = pd.read_pickle(f\"/content/drive/MyDrive/RQE_Data_T20_UK.pkl\")\n",
        "\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='65001'), 'body_Q2']='So when I launch Minecraft, before it finishes loading, it crashes. I do not understand what is going on. Could someone help me? Here is my crash report:'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='36896'), 'body_Q2']='How do I type the infinity symbol in MacTex'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='3031'), 'body_Q2']='Run time error for GP objects'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q1']=='') & (MyData_LLama['userid_Q2']=='65001'), 'body_Q1']='Misplaced allignment tab character line 53'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q1']=='') & (MyData_LLama['userid_Q2']=='16188'), 'body_Q1']='How to Export this animation as a gif file for powerpoint presentation'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q1']=='') & (MyData_LLama['userid_Q2']=='24829'), 'body_Q1']='why does rotation style work on actual coordinates and not variables in tikz 3d plot'\n",
        "\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='50615'), 'body_Q2']='How set a table in margin'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='23835'), 'body_Q2']='Latex equation positioning problem'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='14524'), 'body_Q2']='Chapter comment with regulation'\n",
        "MyData_LLama.loc[(MyData_LLama['body_Q2']=='') & (MyData_LLama['userid_Q2']=='50823'), 'body_Q2']='minipage goes beyond right margin'\n",
        "\n",
        "# # ========== User-aware ==========\n",
        "MyData = pd.concat([\n",
        "    MyData_LLama[['body_Q1', 'body_Q2', 'entailment']],\n",
        "    MyData2['U_Background_kn']\n",
        "    ], axis=1)\n",
        "\n",
        "# ========== User-agnostic ==========\n",
        "# MyData = MyData_LLama[['body_Q1', 'body_Q2', 'entailment']]\n",
        "\n",
        "MyData = MyData.rename(columns={'body_Q1': 'q1', 'body_Q2': 'q2'})\n",
        "display(MyData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uERhLdyy5EGD"
      },
      "outputs": [],
      "source": [
        "class RQEDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len, is_eval):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.is_eval = is_eval\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      row_data = self.data.iloc[index]\n",
        "      prompt = generate_prompt_rqe(row_data, self.is_eval)\n",
        "      prompt_encoding = self.tokenizer(prompt,\n",
        "                                       max_length = self.max_len,\n",
        "                                       padding = 'max_length',\n",
        "                                       truncation = True,\n",
        "                                       add_special_tokens = True,\n",
        "                                       return_tensors = 'pt',\n",
        "                                       )\n",
        "      input_ids = prompt_encoding['input_ids'].squeeze()\n",
        "      attention_mask = prompt_encoding['attention_mask'].squeeze()\n",
        "\n",
        "      if self.is_eval == False:\n",
        "        response_index = get_response_index(input_ids, 'RQE')\n",
        "        if response_index:\n",
        "          start_indexes = [i for i, n in enumerate(input_ids) if n == 1]\n",
        "          labels = torch.cat(\n",
        "              (torch.full((start_indexes[0],), -100),\n",
        "               input_ids[start_indexes[0]:])\n",
        "              ).squeeze()\n",
        "        else:\n",
        "          print('response_index not found')\n",
        "      else:\n",
        "        labels = self.tokenizer(row_data['entailment'] + '</s>',\n",
        "                                add_special_tokens = False,\n",
        "                                truncation = True,\n",
        "                                max_length = 5,\n",
        "                                padding = 'max_length',\n",
        "                                return_tensors='pt',\n",
        "                                )\n",
        "        labels = labels['input_ids'].squeeze()\n",
        "      return {\n",
        "          'input_ids': input_ids,\n",
        "          'attention_mask': attention_mask,\n",
        "          'labels': labels\n",
        "          }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6JoU4PE5EGE"
      },
      "outputs": [],
      "source": [
        "class RQEDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data, tokenizer, script_args):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.per_device_train_batch_size = script_args.per_device_train_batch_size\n",
        "        self.per_device_eval_batch_size = script_args.per_device_eval_batch_size\n",
        "        self.max_len = script_args.max_seq_length\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        len_tr = int(script_args.split_ratio[0] * self.data.shape[0])\n",
        "        len_te = int(script_args.split_ratio[1] * self.data.shape[0])\n",
        "        train_data, test_data = train_test_split(self.data,\n",
        "                                                 test_size=len_te,\n",
        "                                                 random_state=42)\n",
        "\n",
        "        train_data.reset_index(drop=True, inplace=True)\n",
        "        test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        self.train_data = RQEDataset(train_data,\n",
        "                                     self.tokenizer,\n",
        "                                     self.max_len,\n",
        "                                     is_eval=False)\n",
        "        self.test_data = RQEDataset(test_data,\n",
        "                                    self.tokenizer,\n",
        "                                    self.max_len,\n",
        "                                    is_eval=True)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=self.per_device_train_batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.test_data,\n",
        "            sampler = torch.utils.data.SequentialSampler(self.test_data,),\n",
        "            batch_size= self.per_device_eval_batch_size,\n",
        "            num_workers=2\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXp_hjjv5EGF"
      },
      "outputs": [],
      "source": [
        "RQE_DataModule = RQEDataModule(\n",
        "    MyData,\n",
        "    MyModel.tokenizer,\n",
        "    script_args\n",
        ")\n",
        "print(\"num train batches\", len(RQE_DataModule.train_dataloader()))\n",
        "print(\"num test batches\", len(RQE_DataModule.test_dataloader()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "347MaXcj5EGF"
      },
      "source": [
        "## üåª **3.5. training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-apnzEII5EGG"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    logger = logger,\n",
        "    log_every_n_steps = script_args.logging_steps,\n",
        "    max_epochs = script_args.max_epochs,\n",
        "    accumulate_grad_batches = script_args.gradient_accumulation_steps,\n",
        "    num_sanity_val_steps = script_args.num_sanity_val_steps,\n",
        "    callbacks = [OverrideEpochStepCallback(), checkpoint_callback],\n",
        "    default_root_dir= script_args.output_dir + 'Checkpoints',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToyngBOL5EGH"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/LLamalogs/RQE --samples_per_plugin scalars=6000\n",
        "\n",
        "trainer.fit(\n",
        "    MyModel,\n",
        "    datamodule=RQE_DataModule,\n",
        ")\n",
        "\n",
        "# ========== User-aware ==========\n",
        "!cp -r /content/LLamalogs/RQE /content/drive/MyDrive/LLama/LLama_UM\n",
        "\n",
        "# ========== User-agnostic ==========\n",
        "# !cp -r /content/LLamalogs/RQE /content/drive/MyDrive/LLama/LLama_WoUM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye5Nv2Ef5EGI"
      },
      "source": [
        "## üåª **3.6. save adapters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsCPnMaM5EGJ"
      },
      "source": [
        "save model in:<br>\n",
        "1.    **local directory** üìÅ   \n",
        " or   <br>\n",
        "2.   **HuggingFace ü§ó Hub**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hodL_0045EGJ"
      },
      "outputs": [],
      "source": [
        "if script_args.save_to == \"Both\" or script_args.save_to == \"Drive\":\n",
        "  MyModel.model.save_pretrained(f\"{Drive_path}LLama/{script_args.adapter_name}\")\n",
        "  print(\"Model successfully saved in \", script_args.output_dir + script_args.adapter_name)\n",
        "\n",
        "if script_args.save_to == \"Both\" or script_args.save_to == \"Hub\":\n",
        "  MyModel.model.push_to_hub(script_args.adapter_name)\n",
        "  print(\"Model successfully saved in \", script_args.adapter_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g_Btf6GvKV6"
      },
      "source": [
        "## üåª **3.7. Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D88WvssEJigB"
      },
      "outputs": [],
      "source": [
        "tokenizer=None\n",
        "trainer=None\n",
        "MyModel = None\n",
        "MyModel2 = None\n",
        "fModel = None\n",
        "BaseModel = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Eq_LuIxQwa2"
      },
      "outputs": [],
      "source": [
        "BaseModel= AutoModelForCausalLM.from_pretrained(\n",
        "    f\"{Drive_path}llama-2-7b-chat-hf\",\n",
        "    device_map={\"\": 0},\n",
        "    offload_folder=\"offload\",\n",
        "    offload_state_dict = True,\n",
        "    # load_in_8bit = True\n",
        "    )\n",
        "\n",
        "# ========== User-agnostic ==========\n",
        "# address = f\"/content/drive/MyDrive/LLama/LLAMA-RQE-WoUM/LLama-RQE10\"\n",
        "\n",
        "# ========== User-aware ==========\n",
        "address = f\"/content/drive/MyDrive/LLama/LLAMA-RQE-UM/LLama-RQE10\"\n",
        "\n",
        "print(\"\\n Loading model from \", address, \"\\n\")\n",
        "config = PeftConfig.from_pretrained(address)\n",
        "fModel= PeftModel.from_pretrained(BaseModel, address, device_map={\"\": 0})\n",
        "fModel = fModel.merge_and_unload()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    f'{Drive_path}llama-2-7b-chat-hf',\n",
        "    padding_side='left'\n",
        "    )\n",
        "tokenizer.pad_token_id = 0\n",
        "\n",
        "\n",
        "fModel.config.pad_token_id = tokenizer.pad_token_id\n",
        "fModel.config.mask_token_id = tokenizer.mask_token_id\n",
        "print(fModel)\n",
        "print(fModel.config)\n",
        "print(\"\\n Model successfully loded from \", address, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vxp5ItKs4mT"
      },
      "outputs": [],
      "source": [
        "def test_step(test_dl):\n",
        "    results = []\n",
        "\n",
        "    for batch in test_dl:\n",
        "        input_ids = batch['input_ids'].cuda()\n",
        "        attention_mask = batch['attention_mask'].cuda()\n",
        "        labels = batch['labels'].cuda()\n",
        "\n",
        "        generated_txts_ids = fModel.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=script_args.max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.97\n",
        "        )\n",
        "\n",
        "        for i in range(input_ids.size(0)):\n",
        "            single_generated_ids = generated_txts_ids[i]\n",
        "\n",
        "            response_start_idx = get_response_index(\n",
        "                single_generated_ids, 'RQE'\n",
        "                )\n",
        "            single_generated_txt = tokenizer.decode(\n",
        "                single_generated_ids[response_start_idx:],\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True\n",
        "            )\n",
        "\n",
        "            single_label_ids = labels[i]\n",
        "            single_label_ids = torch.where(\n",
        "                single_label_ids != -100,\n",
        "                single_label_ids,\n",
        "                tokenizer.pad_token_id\n",
        "            )\n",
        "            single_target_txt = tokenizer.decode(\n",
        "                single_label_ids,\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True\n",
        "            )\n",
        "            results.append([single_generated_txt, single_target_txt])\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpE5FaNjN2PH"
      },
      "outputs": [],
      "source": [
        "Data_RQE = RQEDataModule(\n",
        "    MyData,\n",
        "    tokenizer,\n",
        "    script_args\n",
        "    )\n",
        "print(\"num train batches\", len(Data_RQE.train_dataloader()))\n",
        "print(\"num test batches\", len(Data_RQE.test_dataloader()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G2Ot02LN_2n"
      },
      "outputs": [],
      "source": [
        "fModel.eval()\n",
        "start_time = time.time()\n",
        "test_results = test_step(Data_RQE.test_dataloader())\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "test_results_df = pd.DataFrame(\n",
        "    test_results,\n",
        "    columns = ['predicted_label', 'real_label']\n",
        "    )\n",
        "\n",
        "# ========== User-agnostic ==========\n",
        "# test_results_df.to_pickle(f\"{Drive_path}LLama/Llama_RQE_WoUM_results.pkl\")\n",
        "\n",
        "# ========== User-aware ==========\n",
        "test_results_df.to_pickle(f\"{Drive_path}LLama/Llama_RQE_UM_results.pkl\")\n",
        "\n",
        "test_results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0kvkn4ZUzHw"
      },
      "outputs": [],
      "source": [
        "test_results_df.columns=['generated_label', 'real_label']\n",
        "predicted_labels = test_results_df['generated_label'].apply(lambda x: 0 if x=='Not-entailed' else 1)\n",
        "real_labels = test_results_df['real_label'].apply(lambda x: 0 if x=='Not-entailed' else 1)\n",
        "\n",
        "display(predicted_labels)\n",
        "display(real_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_XINOniv7Hq"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"F1-score: \",\n",
        "    sklearn.metrics.f1_score(\n",
        "        real_labels,\n",
        "        predicted_labels)\n",
        "    )\n",
        "print(\n",
        "    \"Precision: \",\n",
        "    sklearn.metrics.precision_score(\n",
        "        real_labels,\n",
        "        predicted_labels,\n",
        "        average='binary')\n",
        "    )\n",
        "print(\n",
        "    \"Recall: \",\n",
        "    sklearn.metrics.recall_score(\n",
        "        real_labels,\n",
        "        predicted_labels,\n",
        "        average='binary')\n",
        "    )\n",
        "print(\n",
        "    \"Accuracy: \",\n",
        "    sklearn.metrics.accuracy_score(\n",
        "        real_labels,\n",
        "        predicted_labels)\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}